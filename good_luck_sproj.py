# -*- coding: utf-8 -*-
"""good luck sproj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y5vGye-I0ZKserWk0YP1-2WTrWKRKu-I
"""

pip install gensim scikit-learn numpy matplotlib requests

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd /content/drive/My Drive/Colab Notebooks

from gensim.models import KeyedVectors

word2vec = KeyedVectors.load_word2vec_format('/content/drive/My Drive/Colab Notebooks/GoogleNews-vectors-negative300.bin', binary=True)

pip install datasets

import requests, uuid, json, os, re
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull, Delaunay
from datasets import load_dataset
import random
import numpy as np


# azure translator api
def translate_text(texts, from_language="en", to_languages=["fr"]):
    key = "7lZhuv3XnsWVoGWukkpOWUNmx0WYFiJUKuK9r3rdzEyCHKJ9Kyb2JQQJ99AKACYeBjFXJ3w3AAAbACOGnwU2"
    endpoint = "https://api.cognitive.microsofttranslator.com"
    location = "eastus"
    path = '/translate?api-version=3.0'
    url = endpoint + path

    headers = {
        'Ocp-Apim-Subscription-Key': key,
        'Ocp-Apim-Subscription-Region': location,
        'Content-type': 'application/json',
        'X-ClientTraceId': str(uuid.uuid4())
    }

    params = {
        'from': from_language,
        'to': to_languages
    }

    body = [{'text': text} for text in texts]
    response = requests.post(url, headers=headers, params=params, json=body)

    if response.status_code != 200:
        print(f"Error: {response.status_code}, {response.text}")
        raise Exception(f"API call failed with status code {response.status_code}")

    return response.json()

# back-Translation
def back_translate(texts, intermediate_language="fr"):
    # translate to Intermediate Language
    intermediate_result = translate_text(texts, to_languages=[intermediate_language])
    translated_text = [result['translations'][0]['text'] for result in intermediate_result]

    # translate Back to English
    back_result = translate_text(translated_text, from_language=intermediate_language, to_languages=["en"])
    back_translated_text = [result['translations'][0]['text'] for result in back_result]

    return back_translated_text

# data cleaning: make sure all words are consistent and lowercase
def clean_text(sentence):
    if isinstance(sentence, list):
      return[clean_text(item) for item in sentence]
    elif isinstance(sentence,str):
      # lowercase
      sentence = sentence.lower()
      # remove punctuation and special characters
      sentence = re.sub(r'[^\w\s]', '', sentence)
      # remove non-ASCII characters
      # sentence = re.sub(r'[^\x00-\x7F]+', '', sentence)
      # remove extra spaces
      sentence = re.sub(r'\s+', ' ', sentence).strip()
      return sentence
    else:
      raise ValueError("Input must be a string or a list of string.")

def get_all_word_vectors(sentences, model):
    """
    Extracts unique word vectors for all words in a list of sentences using a word embedding model.
    """
    all_vectors = []
    for sentence in sentences:
        words = sentence.split()
        for word in words:
            if word in model and not any(np.array_equal(model[word], existing_vector) for existing_vector in all_vectors):
                all_vectors.append(model[word])
    return all_vectors

def transform_with_pca(all_vectors, pca, label):
    if not all_vectors:
        print(f"No vectors available for {label}. Skipping PCA transformation.")
        return None
    return pca.transform(all_vectors)

def tokenize(sentence):
    return sentence.split()

def select_words_to_replace(tokens, num_words=2):
    return random.sample(tokens, min(num_words, len(tokens)))

def replace(sentence, old_word, new_word):
    return sentence.replace(old_word, new_word, 1)

# world replacement algorithm
#def word_replacement_augmentation(sentences, word_embedding_model, num_similar_words=3, num_words_to_replace=2):
    #all_augmented_sentences = []
    #for sentence in sentences:
      #  tokens = tokenize(sentence)
      #  selected_words = select_words_to_replace(tokens, num_words_to_replace)
      #  augmented_sentences = []
      #  for word in selected_words:
          #  if word in word_embedding_model:
           #     similar_words = word_embedding_model.most_similar(positive=[word], topn=num_similar_words)
             #   for similar_word, _ in similar_words:
              #      augmented_sentences.append(replace(sentence, word, similar_word))
    #    all_augmented_sentences.extend(augmented_sentences)
    #return all_augmented_sentences

#  plot with convex hulls and Delaunay triangulation
def plot_with_convex_hull_and_triangulation(vectors, label, color):
    plt.scatter(vectors[:, 0], vectors[:, 1], label=label, color=color)

    # plot convex hull if there are enough points
    if len(vectors) > 2:  # Convex hull requires at least 3 points
        hull = ConvexHull(vectors)
        for simplex in hull.simplices:
            plt.plot(vectors[simplex, 0], vectors[simplex, 1], color=color)

    # Plot Delaunay triangulation if there are enough points
    if len(vectors) > 2:
        delaunay = Delaunay(vectors)
        for simplex in delaunay.simplices:
            plt.plot(vectors[simplex, 0], vectors[simplex, 1], color=color, alpha=0.7)


def flatten_list(nested_list):
  #check if the list is empty before accessing elements
  if not nested_list:
    return [] #return empty list if input is empty
  return [item for sublist in nested_list for item in sublist] if isinstance(nested_list[0], list) else nested_list

dataset = load_dataset("sst2")

train_data = list(dataset["train"])

sample_size = 100
sampled_data = random.sample(train_data, sample_size)

datasets = [{"Original": [item["sentence"]]} for item in sampled_data]

all_original_vectors = []
all_back_translated1_vectors = []
all_back_translated2_vectors = []
#all_word_replacement_vectors = []

for dataset in datasets:
    original_text = dataset["Original"]

    # Back-Translation
    back_translated1 = back_translate(original_text, "de")
    back_translated2 = back_translate(original_text,"zh-Hans")

    # Word Replacement
   # word_replacement_sentences = word_replacement_augmentation(original_text, word2vec)

    # Cleaning data
    cleaned_original = [clean_text(sentence) for sentence in original_text]
    cleaned_back_translated1 = [clean_text(sentence) for sentence in back_translated1]
    cleaned_back_translated2 = [clean_text(sentence) for sentence in back_translated2]
   # cleaned_word_replacement = [clean_text(sentence) for sentence in word_replacement_sentences]

    # Convert cleaned data into lists of sentences
    cleaned_original_flat = [" ".join(flatten_list(cleaned_original))]
    cleaned_back_translated1_flat = [" ".join(flatten_list(cleaned_back_translated1))]
    cleaned_back_translated2_flat = [" ".join(flatten_list(cleaned_back_translated2))]

    # Extract word vectors
    original_vectors = get_all_word_vectors(cleaned_original_flat, word2vec)
    back_translated1_vectors = get_all_word_vectors(cleaned_back_translated1_flat, word2vec)
    back_translated2_vectors = get_all_word_vectors(cleaned_back_translated2_flat, word2vec)


    # Append Vectors
    all_original_vectors.extend(original_vectors)
    all_back_translated1_vectors.extend(back_translated1_vectors)
    all_back_translated2_vectors.extend(back_translated2_vectors)
  #  all_word_replacement_vectors.extend(word_replacement_vectors)

# Apply PCA to reduce dimensionality
if len(all_original_vectors) > 2:
    pca = PCA(n_components=2)
    original_reduced = pca.fit_transform(all_original_vectors)
    back_translated1_reduced = pca.transform(all_back_translated1_vectors)
    back_translated2_reduced = pca.transform(all_back_translated2_vectors)
  #  word_replacement_reduced = pca.transform(all_word_replacement_vectors)

    plt.figure(figsize=(10, 6))
    plot_with_convex_hull_and_triangulation(original_reduced, "Original Words", "blue")
    plot_with_convex_hull_and_triangulation(back_translated1_reduced, "Back-Translated 1 Words", "green")
    plt.title("Combined Dataset: Original vs. Back-Translated 1")
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 6))
    plot_with_convex_hull_and_triangulation(original_reduced, "Original Words", "blue")
    plot_with_convex_hull_and_triangulation(back_translated2_reduced, "Back-Translated 2 Words", "orange")
    plt.title("Combined Dataset: Original vs. Back-Translated 2")
    plt.legend()
    plt.show()

   # plt.figure(figsize=(10, 6))
   # plot_with_convex_hull_and_triangulation(original_reduced, "Original Words", "blue")
   # plot_with_convex_hull_and_triangulation(word_replacement_reduced, "Word Replacement Words", "purple")
   # plt.title("Combined Dataset: Original vs. Word Replacement")
   # plt.legend()
   # plt.show()
else:
    print("Not enough points for PCA.")

from sentence_transformers import SentenceTransformer

# Load the SentenceTransformer model
sentence_model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to embed sentences using SBERT
def sentence_embeddings(sentences):
    return sentence_model.encode(sentences, convert_to_tensor=False)

# Add to your dataset processing loop
all_sentence_original_embeddings = []
all_sentence_back_translated1_embeddings = []
all_sentence_back_translated2_embeddings = []

for dataset in datasets:
    original_text = dataset["Original"]

    # Back-Translation (already defined)
    back_translated1 = [back_translate(sentence, "de") for sentence in original_text]
    back_translated2 = [back_translate(sentence, "zh-Hans") for sentence in original_text]

    # Compute Sentence Embeddings
    original_sentence_emb = sentence_embeddings(original_text)
    back_translated1_sentence_emb = sentence_embeddings(back_translated1)
    back_translated2_sentence_emb = sentence_embeddings(back_translated2)

    # Collect all embeddings
    all_sentence_original_embeddings.extend(original_sentence_emb)
    all_sentence_back_translated1_embeddings.extend(back_translated1_sentence_emb)
    all_sentence_back_translated2_embeddings.extend(back_translated2_sentence_emb)

# Apply PCA to reduce dimensionality for sentence embeddings
if len(all_sentence_original_embeddings) > 2:
    pca_sent = PCA(n_components=2)
    original_sentence_reduced = pca_sent.fit_transform(all_sentence_original_embeddings)
    back_translated1_sentence_reduced = pca_sent.transform(all_sentence_back_translated1_embeddings)
    back_translated2_sentence_reduced = pca_sent.transform(all_sentence_back_translated2_embeddings)

    # Plot Sentence Embeddings for Back-Translated 1
    plt.figure(figsize=(10, 6))
    plot_with_convex_hull_and_triangulation(original_sentence_reduced, "Original Sentences", "blue")
    plot_with_convex_hull_and_triangulation(back_translated1_sentence_reduced, "Back-Translated 1 Sentences", "green")
    plt.title("Sentence Embedding Comparison: Original vs. Back-Translated 1")
    plt.legend()
    plt.show()

    # Plot Sentence Embeddings for Back-Translated 2
    plt.figure(figsize=(10, 6))
    plot_with_convex_hull_and_triangulation(original_sentence_reduced, "Original Sentences", "blue")
    plot_with_convex_hull_and_triangulation(back_translated2_sentence_reduced, "Back-Translated 2 Sentences", "orange")
    plt.title("Sentence Embedding Comparison: Original vs. Back-Translated 2")
    plt.legend()
    plt.show()
else:
    print("Not enough points for PCA on Sentence Embeddings.")
